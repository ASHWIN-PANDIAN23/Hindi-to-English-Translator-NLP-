{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "home = Path.home()\n",
    "desktop = home / \"Desktop\"\n",
    "base_dir = desktop / \"HindiToEnglishMT\"\n",
    "if not base_dir.exists():\n",
    "    base_dir = Path.cwd().resolve().parent\n",
    "print(\"Base dir:\", base_dir)\n",
    "\n",
    "sys.path.append(str(base_dir / \"utils\"))\n",
    "import sentencepiece as spm\n",
    "from model_utils import Seq2SeqTransformer\n",
    "from evaluation_utils import compute_metrics, token_prf1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load test data and best model\n",
    "processed_dir = base_dir / \"data\" / \"processed\"\n",
    "test_path = processed_dir / \"test_tokenized.jsonl\"\n",
    "\n",
    "def load_jsonl(path):\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "test_data = load_jsonl(test_path)\n",
    "print(\"Test examples:\", len(test_data))\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file=str(base_dir / \"models\" / \"vocab\" / \"hi_en_unigram.model\"))\n",
    "pad_id = sp.pad_id(); bos_id = sp.bos_id(); eos_id = sp.eos_id()\n",
    "vocab_size = sp.vocab_size()\n",
    "\n",
    "# Load model\n",
    "ckpt = torch.load(base_dir / \"models\" / \"from_scratch\" / \"best_model.pt\", map_location=device)\n",
    "model = Seq2SeqTransformer(vocab_size=vocab_size, pad_id=pad_id).to(device)\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decode test set (beam search for better quality)\n",
    "from tqdm import tqdm\n",
    "\n",
    "hyps_txt = []\n",
    "refs_txt = []\n",
    "hyp_ids_all = []\n",
    "ref_ids_all = []\n",
    "\n",
    "for ex in tqdm(test_data[:2000], desc=\"Decoding (subset for speed)\"):\n",
    "    # Limit to subset by default; increase/remove slice for full test (may be slow)\n",
    "    src = torch.tensor([ex[\"src\"]], dtype=torch.long, device=device)\n",
    "    out_ids = model.beam_search(src, max_len=128, bos_id=bos_id, eos_id=eos_id, beam_size=4, length_penalty=0.6)\n",
    "    hyp_ids = out_ids[0].tolist()\n",
    "    # Remove initial BOS\n",
    "    if len(hyp_ids) > 0 and hyp_ids[0] == bos_id:\n",
    "        hyp_ids = hyp_ids[1:]\n",
    "    # Cut at EOS\n",
    "    if eos_id in hyp_ids:\n",
    "        hyp_ids = hyp_ids[:hyp_ids.index(eos_id)]\n",
    "    ref_ids = ex[\"tgt\"][1:]  # drop BOS\n",
    "    if eos_id in ref_ids:\n",
    "        ref_ids = ref_ids[:ref_ids.index(eos_id)]\n",
    "\n",
    "    hyp_txt = sp.decode(hyp_ids)\n",
    "    ref_txt = sp.decode(ref_ids)\n",
    "\n",
    "    hyps_txt.append(hyp_txt)\n",
    "    refs_txt.append(ref_txt)\n",
    "    hyp_ids_all.append(hyp_ids)\n",
    "    ref_ids_all.append(ref_ids)\n",
    "\n",
    "print(\"Sample hypothesis:\", hyps_txt[0])\n",
    "print(\"Sample reference: \", refs_txt[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute metrics\n",
    "metrics = compute_metrics(hyps_txt, refs_txt)\n",
    "print(\"Text metrics:\", metrics)\n",
    "\n",
    "# Token-level P/R/F1\n",
    "prf1 = token_prf1(hyp_ids_all, ref_ids_all, pad_id=pad_id)\n",
    "print(\"Token P/R/F1:\", prf1)\n",
    "\n",
    "# Save results\n",
    "import json\n",
    "results_dir = base_dir / \"results\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "with open(results_dir / \"test_metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"text_metrics\": metrics, \"token_prf1\": prf1}, f, indent=2)\n",
    "print(\"Saved metrics to\", results_dir / \"test_metrics.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
