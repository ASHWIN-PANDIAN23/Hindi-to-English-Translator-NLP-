{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Base dir: C:\\Users\\ashwi\\OneDrive\\Desktop\\HindiToEnglishMT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashwi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import platform\n",
    "import torch\n",
    "\n",
    "# Safer memory allocation on Windows (reduce fragmentation)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Project root — save to Desktop/HindiToEnglishMT automatically if exists\n",
    "home = Path.home()\n",
    "desktop = home / \"Desktop\"\n",
    "base_dir = desktop / \"HindiToEnglishMT\"\n",
    "if not base_dir.exists():\n",
    "    # fallback to current working dir\n",
    "    base_dir = Path.cwd().resolve().parent\n",
    "print(\"Base dir:\", base_dir)\n",
    "\n",
    "sys.path.append(str(base_dir / \"utils\"))\n",
    "from dataset_utils import ensure_dirs, load_iitb, ds_to_pairs, clean_pairs, save_pairs_to_csv, split_train_val_test\n",
    "\n",
    "ensure_dirs(base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 100%|██████████| 1659083/1659083 [00:02<00:00, 791737.55 examples/s] \n",
      "Generating validation split: 100%|██████████| 520/520 [00:00<00:00, 76225.42 examples/s]\n",
      "Generating test split: 100%|██████████| 2507/2507 [00:00<00:00, 201493.12 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 1659083\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 520\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2507\n",
      "    })\n",
      "})\n",
      "Raw counts: 1659083 520 2507\n",
      "Cleaned counts: 1603359 520 2506\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load IITB dataset (requires internet the first time)\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"cfilt/iitb-english-hindi\")\n",
    "print(ds)\n",
    "\n",
    "train_pairs = ds_to_pairs(ds[\"train\"])\n",
    "val_pairs = ds_to_pairs(ds[\"validation\"])\n",
    "test_pairs = ds_to_pairs(ds[\"test\"])\n",
    "\n",
    "print(\"Raw counts:\", len(train_pairs), len(val_pairs), len(test_pairs))\n",
    "\n",
    "# Clean and combine\n",
    "train_pairs = clean_pairs(train_pairs)\n",
    "val_pairs = clean_pairs(val_pairs)\n",
    "test_pairs = clean_pairs(test_pairs)\n",
    "\n",
    "print(\"Cleaned counts:\", len(train_pairs), len(val_pairs), len(test_pairs))\n",
    "\n",
    "# (Optional) merge train+val for tokenizer training; keep separate for validation\n",
    "all_for_spm = train_pairs + val_pairs + test_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hi</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें</td>\n",
       "      <td>Give your application an accessibility workout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>एक्सेर्साइसर पहुंचनीयता अन्वेषक</td>\n",
       "      <td>Accerciser Accessibility Explorer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>निचले पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
       "      <td>The default plugin layout for the bottom panel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
       "      <td>The default plugin layout for the top panel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...</td>\n",
       "      <td>A list of plugins that are disabled by default</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  hi  \\\n",
       "0    अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें   \n",
       "1                    एक्सेर्साइसर पहुंचनीयता अन्वेषक   \n",
       "2              निचले पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
       "3               ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
       "4  उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...   \n",
       "\n",
       "                                               en  \n",
       "0  Give your application an accessibility workout  \n",
       "1               Accerciser Accessibility Explorer  \n",
       "2  The default plugin layout for the bottom panel  \n",
       "3     The default plugin layout for the top panel  \n",
       "4  A list of plugins that are disabled by default  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Save cleaned CSVs\n",
    "import pandas as pd\n",
    "processed_dir = base_dir / \"data\" / \"processed\"\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "save_pairs_to_csv(train_pairs, processed_dir / \"train.csv\")\n",
    "save_pairs_to_csv(val_pairs, processed_dir / \"val.csv\")\n",
    "save_pairs_to_csv(test_pairs, processed_dir / \"test.csv\")\n",
    "\n",
    "pd.read_csv(processed_dir / \"train.csv\").head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained SentencePiece saved to: C:\\Users\\ashwi\\OneDrive\\Desktop\\HindiToEnglishMT\\models\\vocab\\hi_en_unigram.model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train a joint SentencePiece model (Unigram or BPE)\n",
    "import sentencepiece as spm\n",
    "from pathlib import Path\n",
    "\n",
    "vocab_dir = base_dir / \"models\" / \"vocab\"\n",
    "vocab_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "spm_input_path = processed_dir / \"spm_corpus.txt\"\n",
    "with open(spm_input_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for hi, en in all_for_spm:\n",
    "        f.write(hi.strip() + \"\\n\")\n",
    "        f.write(en.strip() + \"\\n\")\n",
    "\n",
    "vocab_size = 32000\n",
    "model_prefix = str(vocab_dir / \"hi_en_unigram\")\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=str(spm_input_path),\n",
    "    model_prefix=model_prefix,\n",
    "    vocab_size=vocab_size,\n",
    "    character_coverage=1.0,\n",
    "    model_type=\"unigram\",\n",
    "    input_sentence_size=2000000,\n",
    "    shuffle_input_sentence=True,\n",
    "    bos_id=1, eos_id=2, pad_id=0, unk_id=3\n",
    ")\n",
    "\n",
    "print(\"Trained SentencePiece saved to:\", model_prefix + \".model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs — PAD: 0 BOS: 1 EOS: 2 UNK: 3\n",
      "Saved tokenized splits.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Encode datasets using the trained SentencePiece model\n",
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file=str(vocab_dir / \"hi_en_unigram.model\"))\n",
    "pad_id = sp.pad_id()\n",
    "bos_id = sp.bos_id()\n",
    "eos_id = sp.eos_id()\n",
    "unk_id = sp.unk_id()\n",
    "print(\"Token IDs — PAD:\", pad_id, \"BOS:\", bos_id, \"EOS:\", eos_id, \"UNK:\", unk_id)\n",
    "\n",
    "def encode_pair(hi, en):\n",
    "    # For source (Hindi) we add BOS at start and EOS at end\n",
    "    src = [bos_id] + sp.encode(hi, out_type=int) + [eos_id]\n",
    "    # For target (English) we add BOS/EOS as well\n",
    "    tgt = [bos_id] + sp.encode(en, out_type=int) + [eos_id]\n",
    "    return src, tgt\n",
    "\n",
    "def encode_split(pairs):\n",
    "    src_ids = []\n",
    "    tgt_ids = []\n",
    "    for hi, en in pairs:\n",
    "        s, t = encode_pair(hi, en)\n",
    "        src_ids.append(s)\n",
    "        tgt_ids.append(t)\n",
    "    return src_ids, tgt_ids\n",
    "\n",
    "train_src, train_tgt = encode_split(train_pairs)\n",
    "val_src, val_tgt = encode_split(val_pairs)\n",
    "test_src, test_tgt = encode_split(test_pairs)\n",
    "\n",
    "import json\n",
    "def save_jsonl(path, src_ids, tgt_ids):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for s, t in zip(src_ids, tgt_ids):\n",
    "            f.write(json.dumps({\"src\": s, \"tgt\": t}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "save_jsonl(processed_dir / \"train_tokenized.jsonl\", train_src, train_tgt)\n",
    "save_jsonl(processed_dir / \"val_tokenized.jsonl\", val_src, val_tgt)\n",
    "save_jsonl(processed_dir / \"test_tokenized.jsonl\", test_src, test_tgt)\n",
    "\n",
    "print(\"Saved tokenized splits.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
